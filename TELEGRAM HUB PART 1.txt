# Product Requirements Document: Telegram Face Recognition Archival System

## Document Overview

This Product Requirements Document provides complete technical specifications for implementing an automated biometric archival system integrated with Telegram. The system enables users to scan chat histories, extract faces from media, cluster them by identity, and organize results into Telegram Forum Topics.

This document is structured to feed into AI coding agents and human developers. Each section contains detailed specifications, implementation guidance, and code examples where appropriate.

**Document Structure:**
- Part 1: System Architecture and Database Schema (this document)
- Part 2: Core Processing Pipeline Implementation
- Part 3: Telegram Integration and Topic Management
- Part 4: Dashboard and Manual Correction Interface
- Part 5: Deployment, Configuration, and Operations

---

## Part 1: System Architecture and Database Schema

### 1.1 System Architecture Overview

The system consists of five primary components that operate concurrently within a containerized environment. Understanding how these components interact is essential for successful implementation.

**Component 1: Telegram Account Manager**
This component maintains connections to one or more Telegram user accounts. Each account authenticates using the MTProto protocol through the Telethon library. The manager handles session persistence, connection health monitoring, and graceful reconnection when network issues occur. Session files are stored in a persistent Docker volume to survive container restarts.

**Component 2: Message Scanner and Media Downloader**
This component iterates through chat histories, identifying messages containing photos or videos. It operates in two distinct modes. During initial backfill, it walks backwards through history from the most recent message to the oldest accessible message. During real-time monitoring, it listens for new messages as they arrive. The scanner maintains checkpoint state in the database, allowing resumable processing after interruptions.

**Component 3: Face Processing Pipeline**
This component receives media from the scanner as in-memory buffers. For images, it directly processes the buffer to detect faces and extract embeddings. For videos, it implements adaptive frame extraction, processing either keyframes or fixed-interval frames depending on video characteristics. Each detected face generates a 512-dimensional embedding vector representing that person's biometric signature.

**Component 4: Biometric Database and Clustering Engine**
This component stores face embeddings in PostgreSQL with pgvector extension. When a new face is detected, it performs vector similarity search to determine if this face matches any existing identity. Based on configurable thresholds, it either assigns the face to an existing topic or creates a new identity. The database maintains relationships between embeddings, topics, and source messages.

**Component 5: Streamlit Dashboard Interface**
This component provides a web-based interface for monitoring system operation and manually correcting clustering decisions. Users can view processing statistics, browse identified faces organized by topic, merge duplicate identities, split incorrectly grouped faces, and perform reverse image searches against the database.

**Component Interaction Flow**
The Scanner discovers a message with media and passes it to the Downloader. The Downloader streams the media into memory and hands the buffer to the Processing Pipeline. The Pipeline extracts face embeddings and queries the Database. The Database returns either a matching topic identifier or indicates this is a new identity. The Scanner then uploads the media to the appropriate Telegram topic. Throughout this flow, the Dashboard queries the Database to display current state to users.

### 1.2 Technology Stack Rationale

Each technology choice addresses specific project constraints around minimal disk usage, code simplicity, and processing efficiency.

**Telethon for Telegram Protocol**
Telethon provides direct MTProto access, enabling the system to act as a user rather than a bot. This grants full access to chat history including messages sent before the system was added. Telethon's entity caching and robust InputPeer resolution make it ideal for systems that interact with thousands of chats and users. The library handles Telegram's flood protection mechanisms automatically through exponential backoff.

**InsightFace with buffalo_l Model**
InsightFace offers state-of-the-art face recognition using the ArcFace loss function. The buffalo_l model uses a ResNet50 backbone, providing superior accuracy compared to lightweight alternatives. With approximately 99.8 percent accuracy on standard benchmarks, it minimizes false negatives, which is your stated priority. The model runs efficiently on CPU using ONNX Runtime, avoiding the gigabyte-scale dependencies of TensorFlow or PyTorch.

**PyAV for Video Processing**
PyAV provides Python bindings to FFmpeg, allowing video streams to be decoded entirely in memory from BytesIO buffers. This eliminates disk I/O for temporary video files. PyAV exposes fine-grained control over frame extraction, enabling the adaptive strategy that processes keyframes for long videos and fixed-rate frames for short videos. The library efficiently handles various video codecs and container formats commonly used in Telegram.

**PostgreSQL with pgvector Extension**
PostgreSQL provides ACID-compliant storage with powerful querying capabilities. The pgvector extension adds vector similarity search using IVFFlat indexing, which scales to millions of embeddings with sub-millisecond query times. Unlike pure in-memory solutions like FAISS, pgvector persists data automatically and supports concurrent access from multiple processes. This allows the processing pipeline and dashboard to operate simultaneously without lock contention.

**Streamlit for Dashboard**
Streamlit transforms Python scripts into interactive web applications without requiring frontend development expertise. Its reactive programming model automatically updates the interface when underlying data changes. Streamlit integrates natively with pandas for data manipulation and supports custom components for advanced interactions. This dramatically reduces implementation complexity compared to building a separate React frontend with REST API.

### 1.3 Database Schema Specification

The database schema consists of four core tables and one virtual table for vector storage. Understanding these tables and their relationships is critical for implementing the system correctly.

**Table: telegram_accounts**
This table tracks Telegram user accounts that have been added to the system. Each account can scan different chats and maintains independent progress through history.

```sql
CREATE TABLE telegram_accounts (
    id SERIAL PRIMARY KEY,
    phone_number VARCHAR(20) UNIQUE NOT NULL,
    session_file_path VARCHAR(255) NOT NULL,
    status VARCHAR(20) DEFAULT 'active',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_accounts_status ON telegram_accounts(status);
```

The phone_number field uniquely identifies the account and corresponds to the Telegram account's registered phone number. The session_file_path points to the location in the Docker volume where Telethon stores the session file for this account. The status field indicates whether the account is actively scanning, paused, or encountering errors. The timestamps track when the account was added and when it last successfully communicated with Telegram.

**Table: telegram_topics**
This table maintains the mapping between detected identities and Telegram Forum topics where media for each identity is organized.

```sql
CREATE TABLE telegram_topics (
    id SERIAL PRIMARY KEY,
    topic_id BIGINT UNIQUE NOT NULL,
    label VARCHAR(255) DEFAULT 'Unknown Person',
    first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    exemplar_image_url TEXT,
    embedding_count INTEGER DEFAULT 0,
    message_count INTEGER DEFAULT 0
);

CREATE INDEX idx_topics_label ON telegram_topics(label);
CREATE INDEX idx_topics_last_seen ON telegram_topics(last_seen DESC);

**Table: processed_users**
This table tracks users whose profile photos have been scanned to prevent redundant processing.

```sql
CREATE TABLE processed_users (
    user_id BIGINT PRIMARY KEY,
    last_profile_photo_scan TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    photo_file_id VARCHAR(255)
);
```
```

The topic_id field stores the Telegram message thread identifier returned when creating a forum topic. This is a large integer that Telegram uses to route messages to specific topics within a supergroup. The label field contains the human-assigned name for this identity, initially set to a generic value but updatable through the dashboard. The exemplar_image_url stores a reference to the best representative image for this identity, displayed in the dashboard gallery. The counters track how many embeddings and total messages belong to this identity, useful for dashboard statistics.

**Table: face_embeddings**
This table stores the actual biometric data as 512-dimensional vectors along with metadata about where each face was discovered.

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE face_embeddings (
    id SERIAL PRIMARY KEY,
    topic_id INTEGER REFERENCES telegram_topics(id) ON DELETE CASCADE,
    embedding vector(512) NOT NULL,
    source_chat_id BIGINT NOT NULL,
    source_message_id BIGINT NOT NULL,
    quality_score REAL DEFAULT 0.0,
    detection_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_embeddings_topic ON face_embeddings(topic_id);
CREATE INDEX idx_embeddings_source ON face_embeddings(source_chat_id, source_message_id);
```

The embedding field uses pgvector's vector data type, storing 512 floating-point values representing the face. The topic_id foreign key links this embedding to a specific identity topic. The source fields record which chat and message this face was extracted from, creating a full audit trail. The quality_score captures InsightFace's confidence in the detection, with higher values indicating clearer, better-aligned faces. This score helps select the exemplar image and can weight similarity searches toward higher-quality embeddings.

**Vector Index for Similarity Search**
PostgreSQL requires an explicit index on the embedding column to enable efficient similarity search. The IVFFlat index partitions the vector space into clusters.

```sql
CREATE INDEX idx_embeddings_vector ON face_embeddings 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 1000);
```

The lists parameter specifies the number of clusters. With 1000 clusters, the index provides excellent performance for databases containing up to several million embeddings. The vector_cosine_ops operator class tells PostgreSQL to optimize for cosine similarity distance, which is the standard metric for comparing face embeddings. After creating this index, similarity searches execute in approximately 10 to 30 milliseconds even with hundreds of thousands of vectors.

**Table: scan_checkpoints**
This table implements the resumable processing mechanism. Each row tracks progress through a specific chat's history.

```sql
CREATE TABLE scan_checkpoints (
    id SERIAL PRIMARY KEY,
    account_id INTEGER REFERENCES telegram_accounts(id) ON DELETE CASCADE,
    chat_id BIGINT NOT NULL,
    chat_title VARCHAR(255),
    last_processed_message_id BIGINT,
    total_messages INTEGER DEFAULT 0,
    processed_messages INTEGER DEFAULT 0,
    scan_mode VARCHAR(20) DEFAULT 'backfill',
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(account_id, chat_id)
);

CREATE INDEX idx_checkpoints_account ON scan_checkpoints(account_id);
CREATE INDEX idx_checkpoints_mode ON scan_checkpoints(scan_mode);
```

The account_id and chat_id combination uniquely identifies which chat is being scanned by which account. The last_processed_message_id stores the most recent message that has been successfully processed, allowing the scanner to resume from this point after a restart. The message counters enable progress tracking and estimated time remaining calculations. The scan_mode field differentiates between backfill, which is walking backwards through history, and realtime, which is monitoring for new messages. The last_updated timestamp helps identify stalled scans that may need intervention.

**Table: processing_metrics**
This table captures operational statistics for dashboard display and performance monitoring.

```sql
CREATE TABLE processing_metrics (
    id SERIAL PRIMARY KEY,
    metric_name VARCHAR(100) NOT NULL,
    metric_value BIGINT NOT NULL,
    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_metrics_name_time ON processing_metrics(metric_name, recorded_at DESC);
```

The system periodically inserts rows tracking metrics like images_processed_per_minute, database_query_time_ms, telegram_api_calls_per_minute, and memory_usage_mb. The dashboard queries this table to generate time-series charts showing processing throughput and system health over various time windows.

### 1.4 Database Connection Management

The application requires robust database connection handling to support concurrent access from multiple components while gracefully handling connection failures.

**Connection Pool Configuration**
PostgreSQL connections are expensive to establish. Creating a new connection for every query would severely impact performance. Instead, the application maintains a connection pool that reuses connections across requests.

```python
import psycopg2
from psycopg2 import pool
import os

class DatabasePool:
    """
    Manages a pool of PostgreSQL connections with automatic retry logic.
    The pool is thread-safe and can be safely accessed from asyncio tasks.
    """
    
    def __init__(self):
        self.connection_pool = None
        self._initialize_pool()
    
    def _initialize_pool(self):
        """
        Creates a connection pool with configuration from environment variables.
        The pool maintains between 5 and 20 connections depending on load.
        """
        db_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': int(os.getenv('POSTGRES_PORT', 5432)),
            'database': os.getenv('POSTGRES_DB', 'face_archiver'),
            'user': os.getenv('POSTGRES_USER', 'postgres'),
            'password': os.getenv('POSTGRES_PASSWORD', ''),
        }
        
        self.connection_pool = pool.ThreadedConnectionPool(
            minconn=5,
            maxconn=20,
            **db_config
        )
    
    def get_connection(self):
        """
        Retrieves a connection from the pool. Blocks if all connections are in use.
        The caller is responsible for returning the connection via return_connection().
        """
        return self.connection_pool.getconn()
    
    def return_connection(self, connection):
        """
        Returns a connection to the pool, making it available for reuse.
        Should be called in a finally block to ensure connections are always returned.
        """
        self.connection_pool.putconn(connection)
    
    def close_all(self):
        """
        Closes all connections in the pool during graceful shutdown.
        """
        if self.connection_pool:
            self.connection_pool.closeall()

# Global instance shared across the application
db_pool = DatabasePool()
```

This connection pool pattern ensures efficient database access while preventing resource exhaustion. The minimum connection count of five ensures that common operations never wait for connection establishment. The maximum count of twenty prevents the application from overwhelming the database server during high load.

**Context Manager for Safe Connection Usage**
To prevent connection leaks where connections are acquired but never returned, the application uses a context manager that guarantees proper cleanup even when exceptions occur.

```python
from contextlib import contextmanager

@contextmanager
def get_db_connection():
    """
    Context manager that safely acquires and releases database connections.
    Usage:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT ...")
    The connection is automatically returned to the pool when the block exits.
    """
    conn = db_pool.get_connection()
    try:
        yield conn
    finally:
        db_pool.return_connection(conn)
```

Throughout the codebase, database operations use this context manager to ensure connections are always properly managed. This pattern prevents subtle bugs where connection leaks gradually exhaust the pool and cause the application to hang.

### 1.5 Environment Configuration

The application follows twelve-factor app principles, storing configuration in environment variables rather than hardcoding values. This enables the same codebase to run in development, staging, and production with different configurations.

**Required Environment Variables**
These variables must be set before starting the application. Missing required variables should cause the application to fail fast with a clear error message rather than starting with invalid configuration.

```bash
# Telegram API Credentials
TG_API_ID=12345678
TG_API_HASH=abcdef1234567890abcdef1234567890

# PostgreSQL Database Connection
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=face_archiver
POSTGRES_USER=postgres
POSTGRES_PASSWORD=secure_password_here

# Telegram Hub Group Configuration
HUB_GROUP_ID=-1001234567890

# Processing Configuration
SIMILARITY_THRESHOLD=0.55
MAX_MEDIA_SIZE_MB=200
VIDEO_FRAME_RATE=1.0
VIDEO_KEYFRAME_ONLY=false

# Update Checker Configuration
GITHUB_REPO=username/repo-name
GITHUB_BRANCH=main
UPDATE_CHECK_INTERVAL=1800
```

**Configuration Validation**
The application includes a configuration validator that runs on startup to ensure all required variables are present and have valid values.

```python
import os
import sys

def validate_configuration():
    """
    Validates that all required environment variables are present and valid.
    Exits with error code 1 if configuration is invalid.
    """
    required_vars = [
        'TG_API_ID',
        'TG_API_HASH',
        'POSTGRES_HOST',
        'POSTGRES_DB',
        'POSTGRES_USER',
        'POSTGRES_PASSWORD',
        'HUB_GROUP_ID'
    ]
    
    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"ERROR: Missing required environment variables: {', '.join(missing_vars)}")
        print("Please set these variables before starting the application.")
        sys.exit(1)
    
    # Validate numeric values
    try:
        api_id = int(os.getenv('TG_API_ID'))
        if api_id <= 0:
            raise ValueError("TG_API_ID must be positive")
    except ValueError as e:
        print(f"ERROR: Invalid TG_API_ID: {e}")
        sys.exit(1)
    
    try:
        threshold = float(os.getenv('SIMILARITY_THRESHOLD', '0.55'))
        if not 0 < threshold < 1:
            raise ValueError("SIMILARITY_THRESHOLD must be between 0 and 1")
    except ValueError as e:
        print(f"ERROR: Invalid SIMILARITY_THRESHOLD: {e}")
        sys.exit(1)
    
    print("Configuration validation passed.")
```

This validation runs before any components initialize, ensuring that configuration errors are detected immediately rather than causing mysterious failures later in execution.

---

**End of Part 1**