# Product Requirements Document: Telegram Face Recognition Archival System

## Part 3: Database Operations and Clustering Engine

### 3.1 Similarity Search and Identity Matching

The similarity search component represents the intelligence layer of your system, determining whether a newly detected face belongs to an existing identity or represents a person who has never been seen before. This decision-making process occurs thousands or millions of times throughout the system's operation, making it critical that the implementation is both accurate and efficient.

Understanding how vector similarity works at a conceptual level will help you make informed decisions about threshold tuning and troubleshooting matching problems. When InsightFace generates an embedding, it produces a point in 512-dimensional space. You can think of this as a location defined by 512 different coordinates, though visualizing more than three dimensions is impossible for human intuition. The critical property of these embeddings is that faces belonging to the same person cluster together in this high-dimensional space, while faces of different people are separated by larger distances.

Cosine similarity measures the angle between two vectors in this space. Imagine two arrows pointing from the origin. If they point in exactly the same direction, the angle between them is zero degrees and the cosine similarity is one. If they point in completely opposite directions, the angle is 180 degrees and the cosine similarity is negative one. For faces, embeddings of the same person typically produce cosine similarities above 0.6, often reaching 0.8 or higher for high-quality detections under similar conditions. Embeddings of different people usually produce similarities below 0.4, though people who look similar might score higher.

The threshold you select determines where the system draws the line between "same person" and "different person." Setting the threshold at 0.55, as we have specified for your system, means that any similarity score above 0.55 triggers a match. This relatively permissive threshold aligns with your stated priority of avoiding false negatives. You would rather occasionally group two similar-looking people together, which can be manually corrected through the dashboard, than fail to recognize someone who legitimately appears in your database.

```python
import psycopg2
import numpy as np
from typing import Optional, Tuple, List
import logging
import os

logger = logging.getLogger(__name__)

class IdentityMatcher:
    """
    Performs similarity search against stored embeddings to match faces
    with existing identities or identify new people.
    """
    
    def __init__(self, db_pool):
        self.db_pool = db_pool
        self.similarity_threshold = float(os.getenv('SIMILARITY_THRESHOLD', '0.55'))
        self.min_quality_for_storage = float(os.getenv('MIN_QUALITY_THRESHOLD', '0.5'))
    
    async def find_or_create_identity(
        self, 
        embedding: List[float], 
        quality_score: float,
        source_chat_id: int,
        source_message_id: int
    ) -> Tuple[int, bool]:
        """
        Searches for a matching identity or creates a new one.
        
        This is the core decision point that determines whether a face
        represents someone we've seen before or a new person.
        
        Args:
            embedding: 512-dimensional face embedding vector
            quality_score: Detection quality from InsightFace
            source_chat_id: Telegram chat where this face was found
            source_message_id: Telegram message ID containing this face
        
        Returns:
            Tuple of (topic_id, is_new_identity)
            - topic_id: The Telegram topic ID where this face should be posted
            - is_new_identity: True if a new identity was created, False if matched existing
        """
        # First, check quality threshold
        # We don't want to pollute the database with poor detections
        if quality_score < self.min_quality_for_storage:
            logger.debug(f"Skipping low-quality face (quality: {quality_score:.3f})")
            return None, False
        
        # Search for similar embeddings in the database
        match = await self._search_similar_embeddings(embedding)
        
        if match is not None:
            # Found a matching identity
            topic_id, similarity = match
            logger.info(f"Matched to existing identity (topic {topic_id}) with similarity {similarity:.3f}")
            
            # Store this new embedding linked to the existing identity
            await self._store_embedding(topic_id, embedding, quality_score, source_chat_id, source_message_id)
            
            # Update the identity's last_seen timestamp
            await self._update_identity_last_seen(topic_id)
            
            return topic_id, False
        else:
            # No match found - this is a new person
            logger.info("No match found, creating new identity")
            
            # Create new Telegram topic and database record
            topic_id = await self._create_new_identity()
            
            # Store the embedding for this new identity
            await self._store_embedding(topic_id, embedding, quality_score, source_chat_id, source_message_id)
            
            return topic_id, True
    
    async def _search_similar_embeddings(self, embedding: List[float]) -> Optional[Tuple[int, float]]:
        """
        Performs vector similarity search using pgvector.
        
        The search uses the IVFFlat index to efficiently find the nearest
        neighbor without comparing against every embedding in the database.
        
        Returns:
            Tuple of (topic_id, similarity_score) if match found, None otherwise
        """
        # Convert embedding list to pgvector format
        # PostgreSQL expects vectors as strings like '[0.1, 0.2, ...]'
        embedding_str = '[' + ','.join(map(str, embedding)) + ']'
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # This query uses pgvector's <=> operator for cosine distance
            # Cosine distance = 1 - cosine_similarity, so smaller distance = more similar
            # We limit to 1 result since we only need the closest match
            query = """
                SELECT 
                    t.topic_id,
                    1 - (e.embedding <=> %s::vector) as similarity
                FROM face_embeddings e
                JOIN telegram_topics t ON e.topic_id = t.id
                ORDER BY e.embedding <=> %s::vector
                LIMIT 1
            """
            
            cursor.execute(query, (embedding_str, embedding_str))
            result = cursor.fetchone()
            
            if result is None:
                # Database is empty or search failed
                return None
            
            topic_id, similarity = result
            
            # Check if similarity exceeds our threshold
            if similarity >= self.similarity_threshold:
                return (topic_id, similarity)
            else:
                # Closest match is not similar enough
                logger.debug(f"Closest match has similarity {similarity:.3f}, below threshold {self.similarity_threshold}")
                return None
    
    async def _store_embedding(
        self,
        topic_id: int,
        embedding: List[float],
        quality_score: float,
        source_chat_id: int,
        source_message_id: int
    ):
        """
        Stores an embedding in the database linked to a specific identity.
        
        This creates the permanent record that future similarity searches
        will compare against.
        """
        embedding_str = '[' + ','.join(map(str, embedding)) + ']'
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # Insert into face_embeddings table
            cursor.execute("""
                INSERT INTO face_embeddings 
                    (topic_id, embedding, source_chat_id, source_message_id, quality_score)
                VALUES 
                    ((SELECT id FROM telegram_topics WHERE topic_id = %s), %s::vector, %s, %s, %s)
            """, (topic_id, embedding_str, source_chat_id, source_message_id, quality_score))
            
            # Update the embedding count on the topic
            cursor.execute("""
                UPDATE telegram_topics 
                SET embedding_count = embedding_count + 1
                WHERE topic_id = %s
            """, (topic_id,))
            
            conn.commit()
            
            logger.debug(f"Stored embedding for topic {topic_id}")
    
    async def _update_identity_last_seen(self, topic_id: int):
        """
        Updates the last_seen timestamp for an identity.
        This helps track which identities are actively appearing in new content.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE telegram_topics 
                SET last_seen = CURRENT_TIMESTAMP
                WHERE topic_id = %s
            """, (topic_id,))
            conn.commit()
    
    async def _create_new_identity(self) -> int:
        """
        Creates a new identity record and corresponding Telegram topic.
        This will be fully implemented in section 3.2.
        Returns the new topic_id.
        """
        # Placeholder - detailed implementation follows
        pass
```

The similarity search query leverages pgvector's specialized operators for efficient computation. The `<=>` operator computes cosine distance, which is the complement of cosine similarity. Understanding this relationship is important because the query orders results by distance in ascending order, meaning smaller distances represent more similar faces. The conversion formula `similarity = 1 - distance` transforms the distance back into the similarity metric that is more intuitive to reason about.

The index created earlier in your schema specification dramatically accelerates this search. Without the index, PostgreSQL would need to compute the distance from your query embedding to every single embedding in the database, an operation that scales linearly with database size. With one million embeddings, this brute-force approach takes approximately 500 milliseconds to one second. The IVFFlat index partitions the embedding space into clusters, allowing the database to identify which cluster the query embedding belongs to and then search only within that cluster. This reduces query time to 10 to 30 milliseconds even with millions of embeddings, a performance improvement of 20x to 50x.

The decision to store multiple embeddings per identity rather than maintaining only a single canonical embedding deserves explanation. When someone is first detected and a new identity is created, the system stores their embedding. Subsequent detections of this same person might show them from different angles, under different lighting, or with different expressions. Each of these variations produces a slightly different embedding. By storing all high-quality embeddings rather than just the first one, you create a richer representation of this person that improves matching accuracy for future detections.

Consider a scenario where someone is first photographed in dim indoor lighting, producing a relatively noisy embedding. If you store only this initial embedding, future high-quality photos of this person might not match well because the comparison is against the noisy baseline. By storing the high-quality subsequent detection as well, you improve the chances that future photos will match against at least one of the stored embeddings. The similarity search returns the closest match across all stored embeddings, so having multiple reference points increases robustness.

### 3.2 Telegram Topic Creation and Management

When the identity matcher determines that a detected face represents a new person who has never been seen before, the system must create a Telegram forum topic to serve as this identity's organizational container. Understanding how Telegram's forum topics work at the API level is essential for implementing this correctly.

Telegram forum topics are a relatively recent feature that transforms supergroups into organized discussion spaces. Each topic is essentially a message thread with its own title and icon color. Messages posted to a topic are isolated from other topics, creating separate conversation spaces within a single group. For your use case, each topic represents one person's identity, and all media containing that person's face gets posted to their topic.

Creating a topic requires interacting with Telegram's raw API rather than using high-level Telethon methods, because Telethon's convenience wrappers do not yet fully support all forum functionality. This means constructing the API request manually using Telethon's function calling mechanism.

```python
from telethon import TelegramClient
from telethon.tl.functions.channels import CreateForumTopicRequest
from telethon.tl.types import InputChannel
import logging
import os
import random

logger = logging.getLogger(__name__)

class TopicManager:
    """
    Manages creation and updating of Telegram forum topics.
    Each topic represents one identified person.
    """
    
    def __init__(self, client: TelegramClient, db_pool):
        self.client = client
        self.db_pool = db_pool
        self.hub_group_id = int(os.getenv('HUB_GROUP_ID'))
        
        # Predefined icon colors for visual distinction
        # These are Telegram's standard forum topic colors
        self.icon_colors = [
            0x6FB9F0,  # Blue
            0xFFD67E,  # Yellow
            0xCB86DB,  # Purple
            0x8EEE98,  # Green
            0xFF93B2,  # Pink
            0xFB6F5F,  # Red
        ]
    
    async def create_new_topic(self) -> int:
        """
        Creates a new forum topic in the hub group for a new identity.
        
        Returns:
            The Telegram topic_id (message thread ID) of the created topic
        """
        # Generate a unique label for this identity
        # We'll use sequential numbering, but this can be customized
        label = await self._generate_topic_label()
        
        # Select a random icon color for visual variety
        icon_color = random.choice(self.icon_colors)
        
        logger.info(f"Creating new topic: {label}")
        
        try:
            # Get the InputChannel object for the hub group
            # This is required for the CreateForumTopicRequest
            hub_entity = await self.client.get_input_entity(self.hub_group_id)
            
            # Create the forum topic via Telegram API
            result = await self.client(CreateForumTopicRequest(
                channel=hub_entity,
                title=label,
                icon_color=icon_color,
                random_id=self._generate_random_id()
            ))
            
            # Extract the topic ID from the response
            # The topic ID is actually the ID of the service message that created the topic
            topic_id = result.updates[0].id
            
            logger.info(f"Created topic {topic_id} with label '{label}'")
            
            # Store this topic in the database
            await self._store_topic_in_database(topic_id, label)
            
            return topic_id
            
        except Exception as e:
            logger.error(f"Failed to create forum topic: {e}")
            raise
    
    async def _generate_topic_label(self) -> str:
        """
        Generates a unique label for a new identity.
        Uses sequential numbering: "Person 1", "Person 2", etc.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # Count existing topics to determine the next number
            cursor.execute("SELECT COUNT(*) FROM telegram_topics")
            count = cursor.fetchone()[0]
            
            return f"Person {count + 1}"
    
    def _generate_random_id(self) -> int:
        """
        Generates a random ID for the Telegram API request.
        This is used for request deduplication on Telegram's side.
        """
        return random.randint(1, 2**63 - 1)
    
    async def _store_topic_in_database(self, topic_id: int, label: str):
        """
        Creates a database record for the newly created topic.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO telegram_topics 
                    (topic_id, label, first_seen, last_seen)
                VALUES 
                    (%s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """, (topic_id, label))
            
            conn.commit()
            
            logger.debug(f"Stored topic {topic_id} in database")
    
    async def update_topic_title(self, topic_id: int, new_title: str):
        """
        Updates the title of an existing topic.
        This is called when users rename identities through the dashboard.
        """
        try:
            hub_entity = await self.client.get_input_entity(self.hub_group_id)
            
            from telethon.tl.functions.channels import EditForumTopicRequest
            
            await self.client(EditForumTopicRequest(
                channel=hub_entity,
                topic_id=topic_id,
                title=new_title
            ))
            
            # Update database to match
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE telegram_topics 
                    SET label = %s
                    WHERE topic_id = %s
                """, (new_title, topic_id))
                conn.commit()
            
            logger.info(f"Updated topic {topic_id} title to '{new_title}'")
            
        except Exception as e:
            logger.error(f"Failed to update topic title: {e}")
            raise
    
    async def get_topic_info(self, topic_id: int) -> dict:
        """
        Retrieves information about a topic from the database.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT topic_id, label, first_seen, last_seen, 
                       embedding_count, message_count, exemplar_image_url
                FROM telegram_topics
                WHERE topic_id = %s
            """, (topic_id,))
            
            result = cursor.fetchone()
            
            if result is None:
                return None
            
            return {
                'topic_id': result[0],
                'label': result[1],
                'first_seen': result[2],
                'last_seen': result[3],
                'embedding_count': result[4],
                'message_count': result[5],
                'exemplar_image_url': result[6]
            }
```

The random ID parameter in the CreateForumTopicRequest serves an important purpose in distributed systems. If your application sends a topic creation request but the network connection drops before receiving Telegram's response, you cannot know whether the topic was actually created or not. If you retry the request with a different random ID, you might create duplicate topics. By using the same random ID for retries, Telegram can recognize that this is a duplicate request for an operation that already completed and return the original result instead of creating another topic.

The topic labeling strategy uses simple sequential numbering by default because it provides unique identifiers without requiring additional input. However, the system is designed to support renaming through the dashboard. Users can browse the gallery of detected faces, identify who each person is, and assign meaningful names like "Alice" or "Bob" that replace the generic "Person N" labels. This rename operation updates both the Telegram topic title and the database record to maintain consistency.

### 3.3 Media Upload to Topics

After a face has been matched to an identity and the appropriate topic has been identified or created, the system must upload the original media to that topic. This closes the loop, creating the permanent archive that users can browse in Telegram to see all content featuring each person.

The upload process must handle several challenges. First, the same piece of media might contain multiple faces, requiring upload to multiple different topics. Second, uploads can fail due to network issues or rate limiting, requiring retry logic. Third, the system must track which media has been uploaded to avoid duplicates if processing is interrupted and resumed.

```python
from telethon import TelegramClient
from telethon.tl.types import InputMessagesFilterPhotos, InputMessagesFilterVideo
import asyncio
import logging
from typing import Optional
import io

logger = logging.getLogger(__name__)

class MediaUploader:
    """
    Handles uploading processed media to appropriate Telegram topics.
    Implements retry logic and duplicate prevention.
    """
    
    def __init__(self, client: TelegramClient, db_pool):
        self.client = client
        self.db_pool = db_pool
        self.hub_group_id = int(os.getenv('HUB_GROUP_ID'))
        self.upload_semaphore = asyncio.Semaphore(3)  # Limit concurrent uploads
    
    async def upload_to_topic(
        self,
        topic_id: int,
        media_buffer: io.BytesIO,
        message_id: int,
        chat_id: int,
        caption: Optional[str] = None
    ):
        """
        Uploads media to a specific forum topic with retry logic.
        
        Args:
            topic_id: The Telegram topic ID to upload to
            media_buffer: BytesIO buffer containing the media
            message_id: Original message ID where media was found
            chat_id: Original chat ID where media was found
            caption: Optional caption to attach to the uploaded media
        """
        # Check if we've already uploaded this media to this topic
        if await self._is_already_uploaded(topic_id, message_id, chat_id):
            logger.debug(f"Media from message {message_id} already uploaded to topic {topic_id}")
            return
        
        # Use semaphore to limit concurrent uploads
        # This prevents overwhelming Telegram's API with parallel requests
        async with self.upload_semaphore:
            await self._upload_with_retry(topic_id, media_buffer, message_id, chat_id, caption)
    
    async def _upload_with_retry(
        self,
        topic_id: int,
        media_buffer: io.BytesIO,
        message_id: int,
        chat_id: int,
        caption: Optional[str],
        max_retries: int = 3
    ):
        """
        Uploads media with exponential backoff retry on failure.
        """
        for attempt in range(max_retries):
            try:
                # Construct caption with source information
                full_caption = self._build_caption(message_id, chat_id, caption)
                
                # Reset buffer position before upload
                media_buffer.seek(0)
                
                # Upload to the specific topic by setting reply_to parameter
                # In forum groups, reply_to specifies the topic thread
                message = await self.client.send_file(
                    self.hub_group_id,
                    media_buffer,
                    caption=full_caption,
                    reply_to=topic_id,  # This directs the message to the topic
                    parse_mode='markdown'
                )
                
                logger.info(f"Uploaded media to topic {topic_id}, message {message.id}")
                
                # Record successful upload in database
                await self._record_upload(topic_id, message_id, chat_id, message.id)
                
                # Update message count for the topic
                await self._increment_topic_message_count(topic_id)
                
                return
                
            except FloodWaitError as e:
                # Telegram is rate limiting us
                wait_time = e.seconds
                logger.warning(f"Hit rate limit, waiting {wait_time} seconds")
                await asyncio.sleep(wait_time)
                # Don't count this as a retry attempt
                continue
                
            except Exception as e:
                logger.error(f"Upload attempt {attempt + 1} failed: {e}")
                
                if attempt < max_retries - 1:
                    # Exponential backoff: 1s, 2s, 4s
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                else:
                    # Final attempt failed
                    logger.error(f"Failed to upload after {max_retries} attempts")
                    raise
    
    def _build_caption(self, message_id: int, chat_id: int, user_caption: Optional[str]) -> str:
        """
        Constructs a caption including source metadata.
        This creates an audit trail showing where each piece of media originated.
        """
        parts = []
        
        if user_caption:
            parts.append(user_caption)
        
        # Add source information
        parts.append(f"\nðŸ“ Source: Chat `{chat_id}`, Message `{message_id}`")
        
        return '\n'.join(parts)
    
    async def _is_already_uploaded(self, topic_id: int, message_id: int, chat_id: int) -> bool:
        """
        Checks if media from this source message has already been uploaded to this topic.
        Prevents duplicate uploads if processing is restarted.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT 1 FROM uploaded_media
                WHERE topic_id = %s 
                AND source_message_id = %s 
                AND source_chat_id = %s
                LIMIT 1
            """, (topic_id, message_id, chat_id))
            
            return cursor.fetchone() is not None
    
    async def _record_upload(self, topic_id: int, source_message_id: int, source_chat_id: int, uploaded_message_id: int):
        """
        Records that media has been uploaded to prevent duplicates.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO uploaded_media 
                    (topic_id, source_message_id, source_chat_id, uploaded_message_id, uploaded_at)
                VALUES 
                    (%s, %s, %s, %s, CURRENT_TIMESTAMP)
                ON CONFLICT (topic_id, source_message_id, source_chat_id) DO NOTHING
            """, (topic_id, source_message_id, source_chat_id, uploaded_message_id))
            
            conn.commit()
    
    async def _increment_topic_message_count(self, topic_id: int):
        """
        Increments the message counter for a topic.
        This count is displayed in the dashboard.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE telegram_topics 
                SET message_count = message_count + 1
                WHERE topic_id = %s
            """, (topic_id,))
            
            conn.commit()
```

The upload tracking table prevents a subtle but important problem. Imagine the system processes a message containing three faces, uploads the media to all three topics successfully, but then crashes before updating the checkpoint. When the system restarts and reprocesses that message, it would detect the same three faces and attempt to upload the media again, creating duplicates in all three topics. By maintaining a record of which source messages have been uploaded to which topics, the system can recognize that these uploads have already occurred and skip them.

The rate limiting handling deserves special attention because Telegram enforces strict limits on API usage to prevent spam and abuse. When your application exceeds these limits, Telegram responds with a FloodWaitError specifying how many seconds you must wait before making more requests. The exponential backoff strategy handles transient network errors, while the FloodWaitError handling specifically addresses Telegram's rate limiting. These are different types of failures requiring different responses.

The semaphore limiting concurrent uploads to three prevents creating a situation where dozens of upload requests are queued simultaneously. If uploads are proceeding smoothly, limiting concurrency has no effect because uploads complete quickly and new ones begin immediately. However, if Telegram starts rate limiting your application, the semaphore prevents an explosion of queued requests that would take hours to process once the rate limit clears. Instead, uploads proceed at a controlled pace that stays under Telegram's limits.

### 3.4 Completing the Processing Pipeline

With all the individual components now implemented, we can complete the processing pipeline by connecting the face detection output to the database operations and media uploads. This integration happens in the `_process_face` method that was left as a placeholder in Part 2.

```python
class ProcessingQueue:
    """
    Extension of the ProcessingQueue class from Part 2.
    This completes the _process_face implementation.
    """
    
    def __init__(
        self, 
        download_manager: MediaDownloadManager,
        face_processor: FaceProcessor,
        video_extractor: VideoFrameExtractor,
        identity_matcher: IdentityMatcher,
        topic_manager: TopicManager,
        media_uploader: MediaUploader,
        db_pool,
        num_workers: int = 3
    ):
        # Store all components
        self.download_manager = download_manager
        self.face_processor = face_processor
        self.video_extractor = video_extractor
        self.identity_matcher = identity_matcher
        self.topic_manager = topic_manager
        self.media_uploader = media_uploader
        self.db_pool = db_pool
        
        self.queue = Queue(maxsize=1000)
        self.stats = {
            'images_processed': 0,
            'videos_processed': 0,
            'faces_detected': 0,
            'new_identities_created': 0,
            'processing_errors': 0
        }
        
        self.num_workers = num_workers
        self.workers = []
        self.running = False
    
    async def _process_face(
        self, 
        face: dict, 
        message, 
        chat_id: int,
        media_buffer: io.BytesIO,
        all_topic_ids: set
    ):
        """
        Handles a single detected face: identity matching, database storage, and upload.
        
        Args:
            face: Dictionary containing embedding, quality, bbox, etc.
            message: Original Telegram message object
            chat_id: Chat ID where message was found
            media_buffer: BytesIO containing the media file
            all_topic_ids: Set to collect all topic IDs this media should be uploaded to
        """
        try:
            # Step 1: Find or create identity for this face
            result = await self.identity_matcher.find_or_create_identity(
                embedding=face['embedding'],
                quality_score=face['quality'],
                source_chat_id=chat_id,
                source_message_id=message.id
            )
            
            if result is None:
                # Face quality too low, skip
                return
            
            topic_id, is_new = result
            
            if is_new:
                self.stats['new_identities_created'] += 1
                logger.info(f"Created new identity with topic {topic_id}")
            
            # Add this topic to the set of topics to upload to
            all_topic_ids.add(topic_id)
            
            # Step 2: Check if this should be the exemplar image for this identity
            await self._update_exemplar_if_better(topic_id, face, message)
            
        except Exception as e:
            logger.error(f"Error processing face: {e}")
            self.stats['processing_errors'] += 1
    
    async def _update_exemplar_if_better(self, topic_id: int, face: dict, message):
        """
        Updates the exemplar image for an identity if this face is higher quality
        than the current exemplar.
        
        The exemplar is the representative image shown in the dashboard gallery.
        We want it to be the clearest, best-aligned face available.
        """
        # Get current exemplar quality
        current_exemplar = await self._get_current_exemplar(topic_id)
        
        # If no exemplar exists, or this face is higher quality, update
        if current_exemplar is None or face['quality'] > current_exemplar['quality']:
            # Generate a URL or file reference for this image
            exemplar_url = f"t.me/c/{abs(message.chat_id)}/{message.id}"
            
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE telegram_topics
                    SET exemplar_image_url = %s
                    WHERE topic_id = %s
                """, (exemplar_url, topic_id))
                conn.commit()
            
            logger.debug(f"Updated exemplar for topic {topic_id}")
    
    async def _get_current_exemplar(self, topic_id: int) -> Optional[dict]:
        """
        Retrieves the current exemplar information for a topic.
        """
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # Get the highest quality embedding for this topic
            cursor.execute("""
                SELECT quality_score, exemplar_image_url
                FROM face_embeddings e
                JOIN telegram_topics t ON e.topic_id = t.id
                WHERE t.topic_id = %s
                ORDER BY quality_score DESC
                LIMIT 1
            """, (topic_id,))
            
            result = cursor.fetchone()
            
            if result is None:
                return None
            
            return {
                'quality': result[0],
                'url': result[1]
            }
    
    async def _process_message(self, message, account_id: int, chat_id: int):
        """
        Complete processing pipeline - updated from Part 2 to include uploads.
        """
        try:
            # Step 1: Download media
            media_buffer = await self.download_manager.download_media(message)
            
            if media_buffer is None:
                return
            
            # Step 2: Extract frames
            frames = await self._get_processable_frames(message, media_buffer)
            
            if not frames:
                media_buffer.close()
                return
            
            # Step 3: Detect faces
            all_faces = []
            for frame in frames:
                faces = await self.face_processor.process_image(frame)
                all_faces.extend(faces)
            
            # Update statistics
            if isinstance(message.media, MessageMediaPhoto):
                self.stats['images_processed'] += 1
            else:
                self.stats['videos_processed'] += 1
            
            self.stats['faces_detected'] += len(all_faces)
            
            # Step 4: Filter and deduplicate
            quality_faces = self._filter_quality_faces(all_faces)
            unique_faces = self._deduplicate_faces(quality_faces)
            
            # Step 5: Process each face and collect topic IDs
            topic_ids = set()
            for face in unique_faces:
                await self._process_face(face, message, chat_id, media_buffer, topic_ids)
            
            # Step 6: Upload media to all relevant topics
            for topic_id in topic_ids:
                await self.media_uploader.upload_to_topic(
                    topic_id=topic_id,
                    media_buffer=media_buffer,
                    message_id=message.id,
                    chat_id=chat_id
                )
            
            # Clean up
            media_buffer.close()
            
            logger.info(f"Completed processing message {message.id}: {len(unique_faces)} faces, {len(topic_ids)} topics")
            
        except Exception as e:
            logger.error(f"Failed to process message {message.id}: {e}")
            raise
```

This completed implementation demonstrates how all the components work together to transform a raw Telegram message into organized, searchable archive content. The message flows through downloading, frame extraction, face detection, identity matching, and finally media upload. At each stage, the system maintains state in the database to enable resumption after interruptions and to prevent duplicate processing.

The exemplar selection logic ensures that each identity is represented by its highest-quality detection. When browsing the dashboard gallery, users see clear, well-aligned faces rather than blurry or poorly-lit detections. This improves the user experience significantly because identifying people from poor-quality thumbnails is difficult and frustrating. The system automatically selects the best available representation without requiring manual curation.

### 3.5 Database Schema for Upload Tracking

The upload tracking functionality requires an additional database table that was not included in Part 1. This table should be added to your schema initialization.

```sql
CREATE TABLE uploaded_media (
    id SERIAL PRIMARY KEY,
    topic_id INTEGER REFERENCES telegram_topics(id) ON DELETE CASCADE,
    source_message_id BIGINT NOT NULL,
    source_chat_id BIGINT NOT NULL,
    uploaded_message_id BIGINT NOT NULL,
    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(topic_id, source_message_id, source_chat_id)
);

CREATE INDEX idx_uploaded_source ON uploaded_media(source_chat_id, source_message_id);
CREATE INDEX idx_uploaded_topic ON uploaded_media(topic_id);
```

The unique constraint on the combination of topic, source message, and source chat prevents duplicate uploads. The indexes accelerate the common query pattern of checking whether a specific source message has already been uploaded to a topic, which happens for every piece of media processed.

---

**End of Part 3**