# Product Requirements Document: Telegram Face Recognition Archival System

## Part 4: Dashboard and Manual Correction Interface

### 4.1 Dashboard Architecture and Purpose

The Streamlit dashboard serves as the human oversight layer for your automated face recognition system. While the processing pipeline operates autonomously, scanning chats and clustering faces without human intervention, the dashboard provides essential capabilities that no fully automated system can deliver. Understanding the philosophical purpose of this interface helps clarify what features it should include and how they should be designed.

Face recognition technology, even with state-of-the-art models like the buffalo_l architecture you are using, makes mistakes. Two people who look similar might be grouped together incorrectly. The same person photographed under dramatically different conditions might be split into separate identities. Lighting changes, aging, facial hair, glasses, and countless other factors introduce ambiguity that no algorithm handles perfectly. The dashboard transforms these inevitable errors from permanent data corruption into temporary inconveniences that humans can correct.

Beyond error correction, the dashboard provides visibility into system operation. When your processing pipeline runs for days or weeks scanning millions of messages, you need to understand what it is doing. Is it making progress or stalled? Are faces being detected successfully or is the quality threshold filtering everything out? How many new identities are being discovered per hour? These operational questions cannot be answered by examining log files or querying the database directly. The dashboard presents this information in immediately comprehensible visual formats that allow you to assess system health at a glance.

The reverse image search capability transforms your system from a passive archive into an active investigative tool. Instead of only organizing content that your Telegram accounts can access, the dashboard allows you to upload external images and search for matching faces in your database. This enables use cases like identifying whether a person from a screenshot or photo shared through other channels appears anywhere in your indexed content. For your stated purpose of identifying bad actors and scammers, this search capability is potentially more valuable than the automated archival itself.

Streamlit's architecture makes it particularly well-suited for this application. Traditional web development requires maintaining separate frontend and backend codebases with different programming languages, frameworks, and deployment processes. Changes to data structures or business logic often require coordinated updates across both layers. Streamlit collapses this complexity by rendering Python code directly as web interfaces. Your dashboard logic, database queries, and UI components all live in a single Python file that you can modify and test without context switching between languages or frameworks.

The reactive programming model that Streamlit implements means that when a user interacts with a widget like a dropdown menu or button, Streamlit automatically reruns the relevant portions of your script with the new input values and updates the display. You do not write explicit event handlers or manage state synchronization. This dramatically simplifies development for interfaces that primarily display and manipulate database content, which describes your dashboard perfectly.

### 4.2 Dashboard Main View and Statistics Display

The main dashboard view provides an at-a-glance summary of system operation and current state. This view answers the fundamental questions that anyone monitoring the system would want to know immediately upon opening the interface. How much content has been processed? How many unique people have been identified? Is the system actively processing right now or idle? Are there any errors that need attention?

The implementation structures this information in a hierarchical layout that guides the eye from high-level summary metrics to more detailed breakdowns. Streamlit's column layout capabilities allow you to present multiple related metrics side by side, creating visual groupings that reinforce their relationships.

```python
import streamlit as st
import psycopg2
import pandas as pd
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import os
from contextlib import contextmanager

# Database connection configuration
DB_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'postgres'),
    'port': int(os.getenv('POSTGRES_PORT', 5432)),
    'database': os.getenv('POSTGRES_DB', 'face_archiver'),
    'user': os.getenv('POSTGRES_USER', 'postgres'),
    'password': os.getenv('POSTGRES_PASSWORD', ''),
}

@contextmanager
def get_db_connection():
    """
    Context manager for database connections.
    Ensures connections are properly closed even if errors occur.
    """
    conn = psycopg2.connect(**DB_CONFIG)
    try:
        yield conn
    finally:
        conn.close()

def render_main_dashboard():
    """
    Renders the main dashboard view with system statistics and activity overview.
    This is the landing page users see when opening the dashboard.
    """
    st.title("Face Recognition Archive Dashboard")
    st.markdown("---")
    
    # Fetch core statistics from database
    stats = get_system_statistics()
    
    # Display high-level metrics in columns for visual organization
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="Total Identities",
            value=f"{stats['total_identities']:,}",
            help="Number of unique people identified in the system"
        )
    
    with col2:
        st.metric(
            label="Total Embeddings",
            value=f"{stats['total_embeddings']:,}",
            help="Total face detections stored (multiple per identity)"
        )
    
    with col3:
        st.metric(
            label="Messages Processed",
            value=f"{stats['messages_processed']:,}",
            help="Total messages scanned across all chats"
        )
    
    with col4:
        st.metric(
            label="Active Accounts",
            value=f"{stats['active_accounts']}",
            help="Number of Telegram accounts currently scanning"
        )
    
    st.markdown("---")
    
    # Processing activity section
    st.subheader("Processing Activity")
    
    # Display recent activity timeline
    activity_data = get_recent_activity(hours=24)
    
    if not activity_data.empty:
        # Create an interactive line chart showing processing volume over time
        fig = px.line(
            activity_data,
            x='hour',
            y='faces_detected',
            title='Faces Detected (Last 24 Hours)',
            labels={'hour': 'Time', 'faces_detected': 'Faces Detected'}
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("No processing activity in the last 24 hours")
    
    # Display current scanning status
    st.subheader("Scanning Status")
    scanning_status = get_scanning_status()
    
    if not scanning_status.empty:
        # Show which chats are currently being processed
        st.dataframe(
            scanning_status,
            column_config={
                'chat_title': 'Chat',
                'scan_mode': 'Mode',
                'progress_pct': st.column_config.ProgressColumn(
                    'Progress',
                    format='%.1f%%',
                    min_value=0,
                    max_value=100
                ),
                'last_updated': 'Last Updated'
            },
            hide_index=True
        )
    else:
        st.info("No active scans in progress")
    
    # Recent errors section
    st.subheader("Recent Errors")
    errors = get_recent_errors(limit=5)
    
    if not errors.empty:
        # Display errors in an expandable format to avoid cluttering the view
        for idx, error in errors.iterrows():
            with st.expander(f"Error at {error['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}"):
                st.error(f"**Type:** {error['error_type']}")
                st.code(error['error_message'])
    else:
        st.success("No recent errors")

def get_system_statistics() -> dict:
    """
    Queries the database for high-level system statistics.
    These are relatively expensive queries so they should be cached.
    """
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        # Count total identities
        cursor.execute("SELECT COUNT(*) FROM telegram_topics")
        total_identities = cursor.fetchone()[0]
        
        # Count total embeddings
        cursor.execute("SELECT COUNT(*) FROM face_embeddings")
        total_embeddings = cursor.fetchone()[0]
        
        # Estimate messages processed from checkpoints
        cursor.execute("SELECT SUM(processed_messages) FROM scan_checkpoints")
        messages_processed = cursor.fetchone()[0] or 0
        
        # Count active accounts
        cursor.execute("SELECT COUNT(*) FROM telegram_accounts WHERE status = 'active'")
        active_accounts = cursor.fetchone()[0]
        
        return {
            'total_identities': total_identities,
            'total_embeddings': total_embeddings,
            'messages_processed': messages_processed,
            'active_accounts': active_accounts
        }

def get_recent_activity(hours: int = 24) -> pd.DataFrame:
    """
    Retrieves processing activity metrics for the specified time window.
    Returns a DataFrame with hourly aggregated statistics.
    """
    with get_db_connection() as conn:
        # Query embeddings created in the last N hours, grouped by hour
        query = """
            SELECT 
                DATE_TRUNC('hour', detection_timestamp) as hour,
                COUNT(*) as faces_detected
            FROM face_embeddings
            WHERE detection_timestamp >= NOW() - INTERVAL '%s hours'
            GROUP BY hour
            ORDER BY hour
        """
        
        df = pd.read_sql(query, conn, params=(hours,))
        return df

def get_scanning_status() -> pd.DataFrame:
    """
    Retrieves current status of all active scans.
    Shows progress, mode (backfill/realtime), and last update time.
    """
    with get_db_connection() as conn:
        query = """
            SELECT 
                c.chat_title,
                c.scan_mode,
                CASE 
                    WHEN c.total_messages > 0 
                    THEN (c.processed_messages::float / c.total_messages * 100)
                    ELSE 0 
                END as progress_pct,
                c.last_updated
            FROM scan_checkpoints c
            JOIN telegram_accounts a ON c.account_id = a.id
            WHERE a.status = 'active'
            ORDER BY c.last_updated DESC
        """
        
        df = pd.read_sql(query, conn)
        return df

def get_recent_errors(limit: int = 10) -> pd.DataFrame:
    """
    Retrieves the most recent processing errors.
    This requires an errors table that logs exceptions.
    """
    try:
        with get_db_connection() as conn:
            query = """
                SELECT 
                    error_timestamp as timestamp,
                    error_type,
                    error_message
                FROM processing_errors
                ORDER BY error_timestamp DESC
                LIMIT %s
            """
            
            df = pd.read_sql(query, conn, params=(limit,))
            return df
    except:
        # If errors table doesn't exist yet, return empty DataFrame
        return pd.DataFrame()
```

The statistics display uses Streamlit's metric component which provides a clean, card-like presentation for individual numbers. The help parameter adds tooltip text that appears when users hover over the metric, providing context about what the number represents without cluttering the main display. This is particularly important for metrics like total embeddings where the distinction between embeddings and identities might not be immediately obvious to casual users.

The activity timeline chart uses Plotly rather than Streamlit's native charting because Plotly provides interactive features like zooming, panning, and hover tooltips that significantly enhance the user experience. When examining processing patterns, being able to zoom into a specific time range or hover over a data point to see exact values transforms the chart from a static visualization into an interactive exploration tool. The cost of this enhanced functionality is minimal since Plotly integrates seamlessly with Streamlit through a single function call.

The scanning status table demonstrates Streamlit's column configuration system which allows you to specify how different data types should be rendered. The progress percentage uses a progress bar visualization rather than displaying raw numbers, making it immediately obvious which scans are near completion and which have barely started. This visual encoding of quantitative data reduces cognitive load because users can assess progress through pattern recognition rather than numerical comparison.

### 4.3 Identity Gallery and Topic Browser

The identity gallery provides a visual overview of all detected people, organized as a grid of thumbnail images with associated metadata. This view serves multiple purposes simultaneously. It allows users to quickly browse discovered identities to understand who appears in their indexed content. It provides a starting point for manual corrections by making it easy to spot duplicates or misclassifications. It also serves as a search interface where users can filter and sort identities by various criteria.

The implementation must handle potentially thousands of identities efficiently. Loading and rendering thousands of images simultaneously would consume excessive memory and create an unusable interface with long load times. Instead, the gallery implements pagination that displays a manageable subset of identities per page and allows users to navigate through pages or search for specific identities.

```python
import streamlit as st
from PIL import Image
import requests
from io import BytesIO
import math

def render_identity_gallery():
    """
    Renders a paginated gallery of all identified people.
    Users can browse, search, and select identities for detailed viewing or editing.
    """
    st.title("Identity Gallery")
    
    # Search and filter controls
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        search_query = st.text_input(
            "Search identities",
            placeholder="Enter name or ID to search...",
            help="Search by identity label or topic ID"
        )
    
    with col2:
        sort_by = st.selectbox(
            "Sort by",
            options=['Last Seen', 'First Seen', 'Name', 'Face Count'],
            help="How to order the gallery display"
        )
    
    with col3:
        items_per_page = st.selectbox(
            "Items per page",
            options=[12, 24, 48, 96],
            index=1,
            help="Number of identities to show per page"
        )
    
    # Fetch identities with current filters
    identities = get_identities(
        search_query=search_query,
        sort_by=sort_by
    )
    
    if identities.empty:
        st.info("No identities found matching your criteria")
        return
    
    # Calculate pagination
    total_items = len(identities)
    total_pages = math.ceil(total_items / items_per_page)
    
    # Page selection
    page = st.number_input(
        "Page",
        min_value=1,
        max_value=total_pages,
        value=1,
        help=f"Showing {total_items} total identities across {total_pages} pages"
    )
    
    # Get items for current page
    start_idx = (page - 1) * items_per_page
    end_idx = start_idx + items_per_page
    page_identities = identities.iloc[start_idx:end_idx]
    
    # Render gallery grid
    # Streamlit columns for grid layout (4 columns for desktop viewing)
    cols_per_row = 4
    
    for idx in range(0, len(page_identities), cols_per_row):
        cols = st.columns(cols_per_row)
        
        for col_idx, col in enumerate(cols):
            identity_idx = idx + col_idx
            
            if identity_idx >= len(page_identities):
                break
            
            identity = page_identities.iloc[identity_idx]
            
            with col:
                render_identity_card(identity)

def render_identity_card(identity: pd.Series):
    """
    Renders a single identity as a card with thumbnail and metadata.
    Clicking the card navigates to the detailed view for that identity.
    """
    # Container for the card with custom styling
    with st.container():
        # Display exemplar image if available
        if identity['exemplar_image_url']:
            try:
                # For Telegram URLs, we might need to fetch through the API
                # This is a simplified version - production would handle authentication
                st.image(
                    identity['exemplar_image_url'],
                    caption=identity['label'],
                    use_container_width=True
                )
            except:
                # Fallback to placeholder if image cannot be loaded
                st.image("https://via.placeholder.com/300x300?text=No+Image", use_container_width=True)
        else:
            st.image("https://via.placeholder.com/300x300?text=No+Image", use_container_width=True)
        
        # Display metadata
        st.markdown(f"**{identity['label']}**")
        st.caption(f"Topic ID: {identity['topic_id']}")
        st.caption(f"{identity['embedding_count']} faces | {identity['message_count']} messages")
        st.caption(f"Last seen: {identity['last_seen'].strftime('%Y-%m-%d')}")
        
        # Action button to view details
        if st.button("View Details", key=f"view_{identity['topic_id']}", use_container_width=True):
            # Set session state to navigate to detail view
            st.session_state.selected_identity = identity['topic_id']
            st.rerun()

def get_identities(search_query: str = "", sort_by: str = "Last Seen") -> pd.DataFrame:
    """
    Retrieves identities from database with optional search filtering and sorting.
    """
    with get_db_connection() as conn:
        # Build query based on filters
        query = """
            SELECT 
                topic_id,
                label,
                first_seen,
                last_seen,
                exemplar_image_url,
                embedding_count,
                message_count
            FROM telegram_topics
        """
        
        params = []
        
        # Add search filter if provided
        if search_query:
            query += " WHERE label ILIKE %s OR CAST(topic_id AS TEXT) LIKE %s"
            search_pattern = f"%{search_query}%"
            params = [search_pattern, search_pattern]
        
        # Add sorting
        sort_mapping = {
            'Last Seen': 'last_seen DESC',
            'First Seen': 'first_seen DESC',
            'Name': 'label ASC',
            'Face Count': 'embedding_count DESC'
        }
        
        query += f" ORDER BY {sort_mapping.get(sort_by, 'last_seen DESC')}"
        
        df = pd.read_sql(query, conn, params=params if params else None)
        return df
```

The gallery implementation uses Streamlit's session state mechanism to handle navigation between the gallery view and detailed identity views. When a user clicks the view details button on an identity card, the code stores that identity's topic ID in session state and triggers a rerun. The main application logic detects this state change and renders the detail view instead of the gallery. This creates a simple but effective navigation system without requiring complex routing infrastructure.

The search functionality implements case-insensitive partial matching using PostgreSQL's ILIKE operator. This allows users to find identities even if they only remember part of a name or only know the topic ID. The search operates on both the human-readable label and the numeric topic identifier because different users might think about identities in different ways. Someone familiar with the Telegram interface might remember topic IDs, while others will naturally search by the names they have assigned.

The pagination system prevents performance degradation as your identity count grows. With twelve identities per page and four across each row, users see three rows of cards that fit comfortably on a standard screen without scrolling. This density balances between showing enough content to make browsing efficient and keeping the page lightweight enough to load quickly. The configurable items per page option accommodates different usage patterns, with power users potentially preferring to see more identities at once even if it means more scrolling.

### 4.4 Identity Detail View and Manual Corrections

When users navigate from the gallery to view a specific identity in detail, they need access to comprehensive information about that identity and tools to correct clustering errors. The detail view shows all faces that have been assigned to this identity, displays the source messages for each detection, and provides controls for renaming the identity, merging it with another identity, or splitting out faces that were incorrectly grouped.

Understanding how merge and split operations work at the database level is crucial for implementing them correctly. A merge operation combines two separate identities into one, which requires transferring all embeddings and uploaded media from the source identity to the destination identity. A split operation removes selected faces from an identity and creates a new identity for them. Both operations must maintain database consistency and synchronize changes with Telegram's forum topics.

```python
def render_identity_detail():
    """
    Renders detailed view for a specific identity with all associated faces
    and management controls.
    """
    topic_id = st.session_state.get('selected_identity')
    
    if topic_id is None:
        st.error("No identity selected")
        return
    
    # Back button to return to gallery
    if st.button("â† Back to Gallery"):
        del st.session_state.selected_identity
        st.rerun()
    
    # Fetch identity information
    identity = get_identity_details(topic_id)
    
    if identity is None:
        st.error(f"Identity with topic ID {topic_id} not found")
        return
    
    # Header with identity information
    st.title(identity['label'])
    st.caption(f"Topic ID: {identity['topic_id']}")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Faces", identity['embedding_count'])
    
    with col2:
        st.metric("Messages", identity['message_count'])
    
    with col3:
        st.metric("First Seen", identity['first_seen'].strftime('%Y-%m-%d'))
    
    st.markdown("---")
    
    # Management controls in tabs
    tab1, tab2, tab3 = st.tabs(["All Faces", "Rename", "Merge/Split"])
    
    with tab1:
        render_face_gallery(topic_id)
    
    with tab2:
        render_rename_controls(topic_id, identity['label'])
    
    with tab3:
        render_merge_split_controls(topic_id, identity['label'])

def render_face_gallery(topic_id: int):
    """
    Displays all faces that have been assigned to this identity.
    Shows source information for each detection.
    """
    st.subheader("All Detected Faces")
    
    faces = get_identity_faces(topic_id)
    
    if faces.empty:
        st.info("No faces found for this identity")
        return
    
    # Display faces in grid format with metadata
    cols_per_row = 3
    
    for idx in range(0, len(faces), cols_per_row):
        cols = st.columns(cols_per_row)
        
        for col_idx, col in enumerate(cols):
            face_idx = idx + col_idx
            
            if face_idx >= len(faces):
                break
            
            face = faces.iloc[face_idx]
            
            with col:
                # Quality indicator
                quality_color = "ðŸŸ¢" if face['quality_score'] > 0.8 else "ðŸŸ¡" if face['quality_score'] > 0.6 else "ðŸ”´"
                
                st.markdown(f"**Face {face['id']}** {quality_color}")
                st.caption(f"Quality: {face['quality_score']:.2f}")
                st.caption(f"Source: Chat {face['source_chat_id']}, Message {face['source_message_id']}")
                st.caption(f"Detected: {face['detection_timestamp'].strftime('%Y-%m-%d %H:%M')}")
                
                # Checkbox for selection (used in split operation)
                st.checkbox(
                    "Select for split",
                    key=f"select_face_{face['id']}",
                    help="Select faces to move to a new identity"
                )

def render_rename_controls(topic_id: int, current_label: str):
    """
    Provides interface for renaming an identity.
    Updates both database and Telegram topic title.
    """
    st.subheader("Rename Identity")
    
    with st.form("rename_form"):
        new_label = st.text_input(
            "New name",
            value=current_label,
            help="Enter a descriptive name for this person"
        )
        
        submitted = st.form_submit_button("Update Name")
        
        if submitted:
            if new_label.strip() == "":
                st.error("Name cannot be empty")
            elif new_label == current_label:
                st.info("Name unchanged")
            else:
                # Update database and Telegram
                success = update_identity_label(topic_id, new_label)
                
                if success:
                    st.success(f"Identity renamed to '{new_label}'")
                    # Refresh the page to show new name
                    st.rerun()
                else:
                    st.error("Failed to update name")

def render_merge_split_controls(topic_id: int, label: str):
    """
    Provides interface for merging this identity with another or splitting faces out.
    """
    st.subheader("Merge or Split Identity")
    
    # Merge controls
    st.markdown("### Merge with Another Identity")
    st.info("Merging combines all faces from this identity into another identity. This identity will be deleted.")
    
    with st.form("merge_form"):
        # Get list of other identities for merge target selection
        other_identities = get_other_identities(topic_id)
        
        if not other_identities.empty:
            merge_target = st.selectbox(
                "Select identity to merge into",
                options=other_identities['topic_id'].tolist(),
                format_func=lambda tid: f"{other_identities[other_identities['topic_id']==tid]['label'].iloc[0]} (ID: {tid})"
            )
            
            st.warning(f"This will move all faces from '{label}' into the selected identity and delete '{label}'")
            
            confirm = st.checkbox("I understand this action cannot be undone")
            submitted = st.form_submit_button("Merge Identities", type="primary")
            
            if submitted:
                if not confirm:
                    st.error("Please confirm you understand this action")
                else:
                    success = merge_identities(topic_id, merge_target)
                    
                    if success:
                        st.success("Identities merged successfully")
                        # Return to gallery
                        del st.session_state.selected_identity
                        st.rerun()
                    else:
                        st.error("Merge operation failed")
        else:
            st.info("No other identities available to merge with")
    
    st.markdown("---")
    
    # Split controls
    st.markdown("### Split Selected Faces")
    st.info("Splitting moves selected faces into a new identity")
    
    # Check which faces are selected
    faces = get_identity_faces(topic_id)
    selected_face_ids = [
        face['id'] for _, face in faces.iterrows()
        if st.session_state.get(f"select_face_{face['id']}", False)
    ]
    
    if selected_face_ids:
        st.write(f"Selected {len(selected_face_ids)} face(s)")
        
        with st.form("split_form"):
            new_identity_label = st.text_input(
                "Name for new identity",
                placeholder="Enter name for the split-out identity"
            )
            
            submitted = st.form_submit_button("Create New Identity", type="primary")
            
            if submitted:
                if not new_identity_label.strip():
                    st.error("Please provide a name for the new identity")
                else:
                    success = split_faces(selected_face_ids, new_identity_label)
                    
                    if success:
                        st.success(f"Created new identity '{new_identity_label}' with {len(selected_face_ids)} faces")
                        st.rerun()
                    else:
                        st.error("Split operation failed")
    else:
        st.info("Select faces from the 'All Faces' tab to split them into a new identity")

def get_identity_details(topic_id: int) -> dict:
    """
    Retrieves comprehensive information about an identity.
    """
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                topic_id, label, first_seen, last_seen,
                embedding_count, message_count, exemplar_image_url
            FROM telegram_topics
            WHERE topic_id = %s
        """, (topic_id,))
        
        result = cursor.fetchone()
        
        if result is None:
            return None
        
        return {
            'topic_id': result[0],
            'label': result[1],
            'first_seen': result[2],
            'last_seen': result[3],
            'embedding_count': result[4],
            'message_count': result[5],
            'exemplar_image_url': result[6]
        }

def get_identity_faces(topic_id: int) -> pd.DataFrame:
    """
    Retrieves all face embeddings associated with an identity.
    """
    with get_db_connection() as conn:
        query = """
            SELECT 
                e.id,
                e.quality_score,
                e.source_chat_id,
                e.source_message_id,
                e.detection_timestamp
            FROM face_embeddings e
            JOIN telegram_topics t ON e.topic_id = t.id
            WHERE t.topic_id = %s
            ORDER BY e.detection_timestamp DESC
        """
        
        df = pd.read_sql(query, conn, params=(topic_id,))
        return df

def get_other_identities(exclude_topic_id: int) -> pd.DataFrame:
    """
    Retrieves all identities except the specified one.
    Used for merge target selection.
    """
    with get_db_connection() as conn:
        query = """
            SELECT topic_id, label
            FROM telegram_topics
            WHERE topic_id != %s
            ORDER BY label
        """
        
        df = pd.read_sql(query, conn, params=(exclude_topic_id,))
        return df

def update_identity_label(topic_id: int, new_label: str) -> bool:
    """
    Updates an identity's label in both database and Telegram.
    """
    try:
        # Update database
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE telegram_topics
                SET label = %s
                WHERE topic_id = %s
            """, (new_label, topic_id))
            conn.commit()
        
        # Update Telegram topic title
        # This requires the topic manager from the main application
        # For now, we'll log that this needs to happen
        st.info("Note: Telegram topic title will be updated on next sync")
        
        return True
        
    except Exception as e:
        st.error(f"Error updating label: {e}")
        return False

def merge_identities(source_topic_id: int, target_topic_id: int) -> bool:
    """
    Merges all faces and messages from source identity into target identity.
    Deletes the source identity after transfer is complete.
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # Get database IDs for both topics
            cursor.execute("SELECT id FROM telegram_topics WHERE topic_id = %s", (source_topic_id,))
            source_id = cursor.fetchone()[0]
            
            cursor.execute("SELECT id FROM telegram_topics WHERE topic_id = %s", (target_topic_id,))
            target_id = cursor.fetchone()[0]
            
            # Transfer all embeddings from source to target
            cursor.execute("""
                UPDATE face_embeddings
                SET topic_id = %s
                WHERE topic_id = %s
            """, (target_id, source_id))
            
            # Transfer all uploaded media references
            cursor.execute("""
                UPDATE uploaded_media
                SET topic_id = %s
                WHERE topic_id = %s
            """, (target_id, source_id))
            
            # Update target topic counts
            cursor.execute("""
                UPDATE telegram_topics
                SET 
                    embedding_count = (
                        SELECT COUNT(*) FROM face_embeddings WHERE topic_id = %s
                    ),
                    message_count = (
                        SELECT COUNT(*) FROM uploaded_media WHERE topic_id = %s
                    )
                WHERE id = %s
            """, (target_id, target_id, target_id))
            
            # Delete source topic
            cursor.execute("DELETE FROM telegram_topics WHERE id = %s", (source_id,))
            
            conn.commit()
        
        # Note: In production, you would also need to merge Telegram topics
        # This involves moving all messages from source topic to target topic
        # via the Telegram API, which is complex and beyond this scope
        
        return True
        
    except Exception as e:
        st.error(f"Error merging identities: {e}")
        return False

def split_faces(face_ids: list, new_label: str) -> bool:
    """
    Creates a new identity and moves selected faces into it.
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # Create new topic in database
            # Note: In production, this would also create a Telegram topic
            cursor.execute("""
                INSERT INTO telegram_topics (topic_id, label, first_seen, last_seen)
                VALUES (
                    (SELECT MAX(topic_id) + 1 FROM telegram_topics),
                    %s,
                    CURRENT_TIMESTAMP,
                    CURRENT_TIMESTAMP
                )
                RETURNING id, topic_id
            """, (new_label,))
            
            new_id, new_topic_id = cursor.fetchone()
            
            # Move selected faces to new topic
            cursor.execute("""
                UPDATE face_embeddings
                SET topic_id = %s
                WHERE id = ANY(%s)
            """, (new_id, face_ids))
            
            # Update embedding counts
            cursor.execute("""
                UPDATE telegram_topics t
                SET embedding_count = (
                    SELECT COUNT(*) FROM face_embeddings WHERE topic_id = t.id
                )
                WHERE t.id IN (
                    SELECT DISTINCT topic_id FROM face_embeddings WHERE id = ANY(%s)
                ) OR t.id = %s
            """, (face_ids, new_id))
            
            conn.commit()
        
        return True
        
    except Exception as e:
        st.error(f"Error splitting faces: {e}")
        return False
```

The merge operation demonstrates the importance of maintaining referential integrity across multiple tables. When transferring embeddings from one identity to another, you must also transfer the uploaded media references because those references link media posts to the identities they contain. If you only transferred embeddings, the target identity would show an increased embedding count but the message count would not reflect the additional content, creating confusing inconsistencies.

The split operation creates a new identity and transfers selected embeddings to it. This operation is less destructive than merging because it does not delete anything, merely reorganizes the existing data. Users can always merge the split identity back if they realize they made a mistake. The implementation uses PostgreSQL's array operations to efficiently update multiple embedding records in a single query rather than looping through them individually.

Both operations update the embedding and message counts for affected identities. These counts are denormalized data, stored redundantly for display performance. Keeping them accurate requires careful updates whenever embeddings or messages are transferred. An alternative design would calculate these counts dynamically on every query, but this approach degrades performance as the database grows because counting operations scale linearly with table size.

### 4.5 Reverse Image Search

The reverse image search feature allows users to upload an external image and search for matching faces in the database. This transforms the system from a passive archive into an active investigation tool. Understanding how this feature differs from the automated processing pipeline helps clarify its implementation requirements.

When processing Telegram messages, the system handles media that has already been downloaded and buffered in memory by other components. The reverse search feature must handle file uploads from users' local systems, which introduces different error conditions and validation requirements. Users might upload unsupported file formats, corrupted images, or files that are too large. The implementation must validate uploads before attempting to process them.

```python
def render_reverse_search():
    """
    Provides interface for uploading an image and searching for matching faces.
    """
    st.title("Reverse Image Search")
    st.markdown("Upload an image to find matching faces in the database")
    
    uploaded_file = st.file_uploader(
        "Choose an image",
        type=['jpg', 'jpeg', 'png', 'webp'],
        help="Upload a clear photo containing the face you want to find"
    )
    
    if uploaded_file is not None:
        # Display the uploaded image
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)
        
        with col2:
            st.info("Processing image...")
            
            # Process the uploaded image
            results = process_search_image(uploaded_file)
            
            if results is None:
                st.error("Could not detect any faces in the uploaded image")
            elif len(results) == 0:
                st.warning("No matches found in database")
            else:
                st.success(f"Found {len(results)} potential match(es)")
                render_search_results(results)

def process_search_image(uploaded_file) -> list:
    """
    Processes an uploaded image to extract faces and search for matches.
    Returns list of matching identities with similarity scores.
    """
    try:
        # Read uploaded file into PIL Image
        from PIL import Image
        import numpy as np
        
        image = Image.open(uploaded_file)
        
        # Convert to RGB if necessary (handles RGBA, grayscale, etc.)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Convert to NumPy array in BGR format for InsightFace
        image_array = np.array(image)[:, :, ::-1]
        
        # Detect faces in the image
        # This requires importing the FaceProcessor from the main application
        # For this example, we'll show the structure
        face_processor = get_face_processor()
        faces = face_processor._detect_faces(image_array)
        
        if not faces:
            return None
        
        # Search for each detected face
        # Typically we'd expect only one face in a search query image
        all_results = []
        
        for face in faces:
            matches = search_embedding(face['embedding'])
            all_results.extend(matches)
        
        # Sort by similarity score (highest first)
        all_results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # Remove duplicates (might match same identity from multiple faces)
        seen_topics = set()
        unique_results = []
        
        for result in all_results:
            if result['topic_id'] not in seen_topics:
                unique_results.append(result)
                seen_topics.add(result['topic_id'])
        
        return unique_results
        
    except Exception as e:
        st.error(f"Error processing image: {e}")
        return None

def search_embedding(embedding: list, max_results: int = 10) -> list:
    """
    Searches database for faces similar to the provided embedding.
    Returns top N matches with similarity scores.
    """
    embedding_str = '[' + ','.join(map(str, embedding)) + ']'
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        # Find nearest neighbors with similarity scores
        query = """
            SELECT 
                t.topic_id,
                t.label,
                t.exemplar_image_url,
                t.embedding_count,
                1 - (e.embedding <=> %s::vector) as similarity
            FROM face_embeddings e
            JOIN telegram_topics t ON e.topic_id = t.id
            WHERE 1 - (e.embedding <=> %s::vector) > 0.3
            ORDER BY e.embedding <=> %s::vector
            LIMIT %s
        """
        
        cursor.execute(query, (embedding_str, embedding_str, embedding_str, max_results))
        results = cursor.fetchall()
        
        return [
            {
                'topic_id': row[0],
                'label': row[1],
                'exemplar_url': row[2],
                'face_count': row[3],
                'similarity': row[4]
            }
            for row in results
        ]

def render_search_results(results: list):
    """
    Displays search results with similarity scores and links to identities.
    """
    st.markdown("### Search Results")
    
    for result in results:
        with st.container():
            col1, col2, col3 = st.columns([1, 2, 1])
            
            with col1:
                # Display exemplar image if available
                if result['exemplar_url']:
                    try:
                        st.image(result['exemplar_url'], use_container_width=True)
                    except:
                        st.image("https://via.placeholder.com/150", use_container_width=True)
            
            with col2:
                st.markdown(f"**{result['label']}**")
                st.caption(f"Topic ID: {result['topic_id']}")
                st.caption(f"{result['face_count']} faces in database")
                
                # Visual similarity indicator
                similarity_pct = result['similarity'] * 100
                st.progress(result['similarity'])
                st.caption(f"Similarity: {similarity_pct:.1f}%")
            
            with col3:
                # Button to navigate to this identity's detail page
                if st.button("View Identity", key=f"view_search_{result['topic_id']}"):
                    st.session_state.selected_identity = result['topic_id']
                    st.rerun()
            
            st.markdown("---")

@st.cache_resource
def get_face_processor():
    """
    Returns a cached instance of the face processor.
    The @st.cache_resource decorator ensures we only load the model once.
    """
    from face_processor import FaceProcessor
    return FaceProcessor()
```

The reverse search implementation demonstrates Streamlit's caching capabilities through the cache_resource decorator. Loading the InsightFace model takes several seconds and consumes significant memory. Without caching, every search request would reload the model, creating unacceptable latency. The decorator tells Streamlit to load the model once and reuse that instance across all requests, dramatically improving response time for subsequent searches.

The similarity threshold for search results is set lower than the threshold used for automatic clustering. During automated processing, you set the threshold at 0.55 to be inclusive and avoid missing matches. For search results, displaying matches with similarities as low as 0.3 allows users to make their own judgment calls. A similarity of 0.3 indicates these faces are somewhat similar but probably not the same person. However, in investigative scenarios, users might want to examine these marginal matches manually to confirm or rule them out.

The search results display uses a progress bar to visualize similarity scores, making it immediately obvious which results are strong matches versus weak possibilities. The strongest matches appear at the top due to the sorting by similarity score in descending order. This ranking helps users focus their attention on the most likely matches first while still providing access to weaker matches that might be relevant in specific contexts.

### 4.6 Application Entry Point and Navigation

The dashboard application requires a main entry point that coordinates between different views and manages navigation. Streamlit applications use session state to maintain context across reruns, which happen whenever users interact with widgets. Understanding this execution model is essential for building coherent multi-page applications.

```python
import streamlit as st

def main():
    """
    Main entry point for the Streamlit dashboard application.
    Handles navigation between different views based on session state.
    """
    # Configure page settings
    st.set_page_config(
        page_title="Face Recognition Archive",
        page_icon="ðŸ‘¤",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Sidebar navigation
    with st.sidebar:
        st.title("Navigation")
        
        view = st.radio(
            "Select View",
            options=['Dashboard', 'Gallery', 'Search', 'Settings'],
            key='navigation'
        )
        
        st.markdown("---")
        
        # System status in sidebar
        render_sidebar_status()
    
    # Route to appropriate view based on selection
    if 'selected_identity' in st.session_state:
        # If an identity is selected, show detail view regardless of navigation
        render_identity_detail()
    elif view == 'Dashboard':
        render_main_dashboard()
    elif view == 'Gallery':
        render_identity_gallery()
    elif view == 'Search':
        render_reverse_search()
    elif view == 'Settings':
        render_settings()

def render_sidebar_status():
    """
    Displays system status information in the sidebar.
    """
    st.subheader("System Status")
    
    # Quick status check
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM telegram_accounts WHERE status = 'active'")
            active_accounts = cursor.fetchone()[0]
            
            if active_accounts > 0:
                st.success(f"âœ“ {active_accounts} active account(s)")
            else:
                st.warning("âš  No active accounts")
            
            # Check recent activity
            cursor.execute("""
                SELECT COUNT(*) FROM face_embeddings 
                WHERE detection_timestamp > NOW() - INTERVAL '1 hour'
            """)
            recent_detections = cursor.fetchone()[0]
            
            st.metric("Faces (last hour)", recent_detections)
            
    except Exception as e:
        st.error("Cannot connect to database")

def render_settings():
    """
    Provides interface for viewing and modifying system configuration.
    """
    st.title("Settings")
    
    st.subheader("Processing Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.text_input(
            "Similarity Threshold",
            value=os.getenv('SIMILARITY_THRESHOLD', '0.55'),
            help="Threshold for face matching (0-1)",
            disabled=True
        )
    
    with col2:
        st.text_input(
            "Max Media Size (MB)",
            value=os.getenv('MAX_MEDIA_SIZE_MB', '200'),
            help="Maximum file size to process",
            disabled=True
        )
    
    st.info("Configuration changes require restarting the application")
    
    st.markdown("---")
    
    st.subheader("Database Statistics")
    
    render_database_stats()

def render_database_stats():
    """
    Displays detailed database statistics and health information.
    """
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        # Database size
        cursor.execute("""
            SELECT pg_size_pretty(pg_database_size(current_database()))
        """)
        db_size = cursor.fetchone()[0]
        
        st.metric("Database Size", db_size)
        
        # Table sizes
        cursor.execute("""
            SELECT 
                relname as table_name,
                pg_size_pretty(pg_total_relation_size(relid)) as size
            FROM pg_catalog.pg_statio_user_tables
            ORDER BY pg_total_relation_size(relid) DESC
            LIMIT 5
        """)
        
        table_sizes = cursor.fetchall()
        
        st.subheader("Largest Tables")
        for table_name, size in table_sizes:
            st.text(f"{table_name}: {size}")

if __name__ == "__main__":
    main()
```

The navigation system uses Streamlit's radio button widget to create a persistent menu in the sidebar. This menu remains visible regardless of which view is currently displayed, providing consistent access to all major features. The session state mechanism allows the detail view to override the navigation selection when an identity is selected, creating a natural drill-down experience where users can view details and then return to the gallery.

The sidebar status display provides at-a-glance health information without requiring navigation to the full dashboard view. This quick status check helps users determine whether the system is operating normally or requires attention. The live metrics update each time the page reruns, providing reasonably current information without requiring manual refresh.

---

**End of Part 4**